AWSTemplateFormatVersion: '2010-09-09'
Description: 'Enhanced Kinesis Data Firehose for AWS SES Migration with KMS encryption and advanced features'

Parameters:
  ProjectCode:
    Type: String
    Description: Project code for resource naming
    Default: ses-migration

  Environment:
    Type: String
    Description: Environment name
    Default: production
    AllowedValues:
      - development
      - staging
      - production
      - prod
      - dev

  RawLogsBucketName:
    Type: String
    Description: Raw logs S3 bucket name (imported from base stack)

  MaskedLogsBucketName:
    Type: String
    Description: Masked logs S3 bucket name (imported from base stack)

  ErrorLogsBucketName:
    Type: String
    Description: Error logs S3 bucket name (imported from base stack)

  KMSKeyArn:
    Type: String
    Description: KMS Key ARN for encryption (imported from base stack)

  # Additional parameters for Lambda environment variables
  RawLogsBucket:
    Type: String
    Description: Raw logs S3 bucket name for Lambda environment variables

  MaskedLogsBucket:
    Type: String
    Description: Masked logs S3 bucket name for Lambda environment variables

  ErrorLogsBucket:
    Type: String
    Description: Error logs S3 bucket name for Lambda environment variables

  KMSKeyId:
    Type: String
    Description: KMS Key ARN for Lambda environment variables

  BufferSize:
    Type: Number
    Description: 'Firehose buffer size in MB'
    Default: 128
    MinValue: 1
    MaxValue: 128

  BufferInterval:
    Type: Number
    Description: 'Firehose buffer interval in seconds'
    Default: 300
    MinValue: 60
    MaxValue: 900

  CompressionFormat:
    Type: String
    Description: 'Compression format for S3 delivery'
    Default: 'GZIP'
    AllowedValues: ['UNCOMPRESSED', 'GZIP', 'ZIP', 'Snappy']

  RetentionDays:
    Type: Number
    Description: Log retention period in days for S3
    Default: 2555
    MinValue: 1
    MaxValue: 2555


  LambdaRuntime:
    Type: String
    Description: Lambda function runtime
    Default: "python3.13"
    AllowedValues: ["python3.9", "python3.10", "python3.11", "python3.12", "python3.13"]

Conditions:
  IsProduction: !Equals [!Ref Environment, 'production']

Resources:
  # Kinesis Data Stream for unified data source
  UnifiedDataStream:
    Type: AWS::Kinesis::Stream
    Properties:
      Name: !Sub '${ProjectCode}-${Environment}-unified-data-stream'
      ShardCount: 1
      RetentionPeriodHours: 24
      StreamEncryption:
        EncryptionType: KMS
        KeyId: !Ref KMSKeyArn
      Tags:
        - Key: Name
          Value: !Sub '${ProjectCode}-${Environment}-unified-data-stream'
        - Key: Environment
          Value: !Ref Environment
        - Key: Service
          Value: 'ses-migration'
        - Key: DataType
          Value: 'unified'

  # CloudWatch Log Groups for Firehose
  RawLogsFirehoseLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws/kinesisfirehose/${ProjectCode}-${Environment}-raw-logs'
      RetentionInDays: 30

  MaskedLogsFirehoseLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws/kinesisfirehose/${ProjectCode}-${Environment}-masked-logs'
      RetentionInDays: 30

  # IAM Role for Kinesis Data Firehose Transform Lambda
  KinesisTransformLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub '${ProjectCode}-${Environment}-kinesis-transform-role'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
          - Effect: Allow
            Principal:
              Service: firehose.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: 'KinesisTransformPolicy'
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - kms:Decrypt
                  - kms:DescribeKey
                  - kms:Encrypt
                  - kms:GenerateDataKey
                Resource: !Ref KMSKeyArn
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:PutObject
                Resource:
                  - !Sub 'arn:aws:s3:::${RawLogsBucketName}/*'
                  - !Sub 'arn:aws:s3:::${MaskedLogsBucketName}/*'
                  - !Sub 'arn:aws:s3:::${ErrorLogsBucketName}/*'
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: '*'
              - Effect: Allow
                Action:
                  - cloudwatch:PutMetricData
                Resource: '*'
      Tags:
        - Key: Name
          Value: !Sub '${ProjectCode}-${Environment}-kinesis-transform-role'
        - Key: Environment
          Value: !Ref Environment
        - Key: Service
          Value: 'ses-migration'

  # IAM Role for Enhanced Kinesis Data Firehose
  EnhancedDataFirehoseRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub '${ProjectCode}-${Environment}-enhanced-firehose-role'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: firehose.amazonaws.com
            Action: sts:AssumeRole
            Condition:
              StringEquals:
                'sts:ExternalId': !Ref 'AWS::AccountId'
      Policies:
        - PolicyName: 'EnhancedFirehoseDeliveryPolicy'
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:AbortMultipartUpload
                  - s3:GetBucketLocation
                  - s3:GetObject
                  - s3:ListBucket
                  - s3:ListBucketMultipartUploads
                  - s3:PutObject
                Resource:
                  - !Sub 'arn:aws:s3:::${RawLogsBucketName}'
                  - !Sub 'arn:aws:s3:::${RawLogsBucketName}/*'
                  - !Sub 'arn:aws:s3:::${MaskedLogsBucketName}'
                  - !Sub 'arn:aws:s3:::${MaskedLogsBucketName}/*'
                  - !Sub 'arn:aws:s3:::${ErrorLogsBucketName}'
                  - !Sub 'arn:aws:s3:::${ErrorLogsBucketName}/*'
              - Effect: Allow
                Action:
                  - kms:Decrypt
                  - kms:DescribeKey
                  - kms:Encrypt
                  - kms:GenerateDataKey
                  - kms:ReEncrypt*
                Resource: !Ref KMSKeyArn
              - Effect: Allow
                Action:
                  - lambda:InvokeFunction
                  - lambda:GetFunctionConfiguration
                Resource:
                  - !GetAtt DataTransformRawLambda.Arn
                  - !GetAtt DataTransformMaskedLambda.Arn
              - Effect: Allow
                Action:
                  - kinesis:DescribeStream
                  - kinesis:GetShardIterator
                  - kinesis:GetRecords
                  - kinesis:ListShards
                Resource: !GetAtt UnifiedDataStream.Arn
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: '*'
      Tags:
        - Key: Name
          Value: !Sub '${ProjectCode}-${Environment}-enhanced-firehose-role'
        - Key: Environment
          Value: !Ref Environment
        - Key: Service
          Value: 'ses-migration'

  # CloudWatch Log Group for Raw Logs Firehose
  RawLogsFirehoseLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws/kinesisfirehose/${ProjectCode}-${Environment}-raw-logs'
      RetentionInDays: 30
      Tags:
        - Key: Name
          Value: !Sub '${ProjectCode}-${Environment}-raw-logs-firehose-log-group'
        - Key: Environment
          Value: !Ref Environment

  # CloudWatch Log Stream for Raw Logs Firehose
  RawLogsFirehoseLogStream:
    Type: AWS::Logs::LogStream
    Properties:
      LogGroupName: !Ref RawLogsFirehoseLogGroup
      LogStreamName: !Sub '${ProjectCode}-${Environment}-raw-logs-stream'

  # CloudWatch Log Group for Masked Logs Firehose
  MaskedLogsFirehoseLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws/kinesisfirehose/${ProjectCode}-${Environment}-masked-logs'
      RetentionInDays: 30
      Tags:
        - Key: Name
          Value: !Sub '${ProjectCode}-${Environment}-masked-logs-firehose-log-group'
        - Key: Environment
          Value: !Ref Environment

  # CloudWatch Log Stream for Masked Logs Firehose
  MaskedLogsFirehoseLogStream:
    Type: AWS::Logs::LogStream
    Properties:
      LogGroupName: !Ref MaskedLogsFirehoseLogGroup
      LogStreamName: !Sub '${ProjectCode}-${Environment}-masked-logs-stream'

  # Lambda Permission for Raw Data Transformation
  DataTransformRawLambdaPermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref DataTransformRawLambda
      Action: lambda:InvokeFunction
      Principal: firehose.amazonaws.com
      SourceArn: !Sub 'arn:aws:kinesis:${AWS::Region}:${AWS::AccountId}:deliverystream/${ProjectCode}-${Environment}-raw-logs-stream'

  # Lambda Function for Raw Data Transformation
  DataTransformRawLambda:
    Type: AWS::Lambda::Function
    DependsOn: KinesisTransformLambdaRole
    Properties:
      FunctionName: !Sub '${ProjectCode}-${Environment}-data-transform-raw'
      Runtime: !Ref LambdaRuntime
      Handler: index.lambda_handler
      Role: !GetAtt KinesisTransformLambdaRole.Arn
      Timeout: 300
      MemorySize: 512
      Environment:
        Variables:
          ENVIRONMENT: !Ref Environment
          STREAM_TYPE: 'raw'
          KMS_KEY_ID: !Ref KMSKeyId
          RAW_LOGS_BUCKET: !Ref RawLogsBucket
          MASKED_LOGS_BUCKET: !Ref MaskedLogsBucket
          ERROR_LOGS_BUCKET: !Ref ErrorLogsBucket
      Code:
        ZipFile: |
          import json
          import boto3
          import re
          import os
          import gzip
          import base64
          from datetime import datetime

          def lambda_handler(event, context):
              """
              Enhanced data transformation with dual output: raw and masked data
              """
              output = []

              for record in event['records']:
                  try:
                      # Decode the data
                      compressed_payload = base64.b64decode(record['data'])

                      # Handle gzip compressed data
                      try:
                          payload = gzip.decompress(compressed_payload).decode('utf-8')
                      except:
                          # If not compressed, try direct decode
                          payload = compressed_payload.decode('utf-8')

                      # Parse JSON if possible
                      try:
                          log_data = json.loads(payload)
                      except:
                          log_data = {'raw_message': payload}

                      # Determine which version to output based on the delivery stream
                      stream_type = os.environ.get('STREAM_TYPE', 'raw')  # Default to raw

                      if stream_type == 'masked':
                          # Always output masked data for masked stream
                          output_data = json.dumps(mask_sensitive_data(log_data), ensure_ascii=True)
                      else:
                          # Always output raw data for raw stream
                          output_data = json.dumps(log_data, ensure_ascii=True)

                      # Return uncompressed data for Firehose
                      output_record = {
                          'recordId': record['recordId'],
                          'result': 'Ok',
                          'data': base64.b64encode(output_data.encode('utf-8')).decode('utf-8')
                      }

                      output.append(output_record)

                      # Send metrics
                      send_metrics(stream_type, len(output_data))

                  except Exception as e:
                      print(f"Error processing record {record['recordId']}: {str(e)}")

                      # Return error record for Firehose error handling
                      output.append({
                          'recordId': record['recordId'],
                          'result': 'ProcessingFailed'
                      })

              return {'records': output}


          def mask_sensitive_data(data):
              """
              Enhanced data masking for personal information protection
              """
              if isinstance(data, dict):
                  masked_data = {}
                  for key, value in data.items():
                      if isinstance(value, str):
                          masked_data[key] = mask_string_data(value)
                      elif isinstance(value, (dict, list)):
                          masked_data[key] = mask_sensitive_data(value)
                      else:
                          masked_data[key] = value
                  return masked_data
              elif isinstance(data, list):
                  return [mask_sensitive_data(item) for item in data]
              elif isinstance(data, str):
                  return mask_string_data(data)
              else:
                  return data

          def mask_string_data(data):
              """
              Apply string-level masking for email addresses and IP addresses
              """
              # Email address masking - complete domain masking
              email_pattern = r'\b([a-zA-Z0-9._%+-]+)@([a-zA-Z0-9.-]+\.[a-zA-Z]{2,})\b'

              def mask_email(match):
                  local = match.group(1)
                  domain = match.group(2)

                  # Local part masking (first character + asterisks)
                  if len(local) <= 1:
                      masked_local = local
                  else:
                      masked_local = local[0] + '*' * (len(local) - 1)

                  # Complete domain masking for AUTO version
                  masked_domain = '***.***'

                  return f"{masked_local}@{masked_domain}"

              masked_data = re.sub(email_pattern, mask_email, data)

              # IP address masking - show only first octet
              ip_pattern = r'\b(\d{1,3})\.(\d{1,3})\.(\d{1,3})\.(\d{1,3})\b'
              masked_data = re.sub(ip_pattern, r'\1.*.*.*', masked_data)

              return masked_data

          def send_metrics(access_level, data_size):
              """
              Send processing metrics to CloudWatch
              """
              try:
                  cloudwatch = boto3.client('cloudwatch')
                  cloudwatch.put_metric_data(
                      Namespace='SES/DataTransform',
                      MetricData=[
                          {
                              'MetricName': 'ProcessedDataSize',
                              'Value': data_size,
                              'Unit': 'Bytes',
                              'Dimensions': [
                                  {
                                      'Name': 'AccessLevel',
                                      'Value': access_level
                                  },
                                  {
                                      'Name': 'Environment',
                                      'Value': os.environ['ENVIRONMENT']
                                  }
                              ]
                          },
                          {
                              'MetricName': 'ProcessedRecords',
                              'Value': 1,
                              'Unit': 'Count',
                              'Dimensions': [
                                  {
                                      'Name': 'AccessLevel',
                                      'Value': access_level
                                  },
                                  {
                                      'Name': 'Environment',
                                      'Value': os.environ['ENVIRONMENT']
                                  }
                              ]
                          }
                      ]
                  )
              except Exception as e:
                  print(f"Failed to send metrics: {str(e)}")
      Tags:
        - Key: Name
          Value: !Sub '${ProjectCode}-${Environment}-data-transform-raw'
        - Key: Environment
          Value: !Ref Environment
        - Key: Service
          Value: 'ses-migration'

  # Lambda Permission for Masked Data Transformation
  DataTransformMaskedLambdaPermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref DataTransformMaskedLambda
      Action: lambda:InvokeFunction
      Principal: firehose.amazonaws.com
      SourceArn: !Sub 'arn:aws:kinesis:${AWS::Region}:${AWS::AccountId}:deliverystream/${ProjectCode}-${Environment}-masked-logs-stream'

  # Lambda Function for Masked Data Transformation
  DataTransformMaskedLambda:
    Type: AWS::Lambda::Function
    DependsOn: KinesisTransformLambdaRole
    Properties:
      FunctionName: !Sub '${ProjectCode}-${Environment}-data-transform-masked'
      Runtime: !Ref LambdaRuntime
      Handler: index.lambda_handler
      Role: !GetAtt KinesisTransformLambdaRole.Arn
      Timeout: 300
      MemorySize: 512
      Environment:
        Variables:
          ENVIRONMENT: !Ref Environment
          RAW_LOGS_BUCKET: !Ref RawLogsBucketName
          MASKED_LOGS_BUCKET: !Ref MaskedLogsBucketName
          ERROR_LOGS_BUCKET: !Ref ErrorLogsBucketName
          KMS_KEY_ID: !Ref KMSKeyArn
          STREAM_TYPE: 'masked'
      Code:
        ZipFile: |
          import json
          import boto3
          import re
          import os
          import gzip
          import base64
          from datetime import datetime

          def lambda_handler(event, context):
              """
              Enhanced data transformation with dual output: raw and masked data
              """
              output = []

              for record in event['records']:
                  try:
                      # Decode the data
                      compressed_payload = base64.b64decode(record['data'])

                      # Handle gzip compressed data
                      try:
                          payload = gzip.decompress(compressed_payload).decode('utf-8')
                      except:
                          # If not compressed, try direct decode
                          payload = compressed_payload.decode('utf-8')

                      # Parse JSON if possible
                      try:
                          log_data = json.loads(payload)
                      except:
                          log_data = {'raw_message': payload}

                      # Determine which version to output based on the delivery stream
                      stream_type = os.environ.get('STREAM_TYPE', 'raw')  # Default to raw

                      if stream_type == 'masked':
                          # Always output masked data for masked stream
                          output_data = json.dumps(mask_sensitive_data(log_data), ensure_ascii=True)
                      else:
                          # Always output raw data for raw stream
                          output_data = json.dumps(log_data, ensure_ascii=True)

                      # Return uncompressed data for Firehose
                      output_record = {
                          'recordId': record['recordId'],
                          'result': 'Ok',
                          'data': base64.b64encode(output_data.encode('utf-8')).decode('utf-8')
                      }

                      output.append(output_record)

                      # Send metrics
                      send_metrics(stream_type, len(output_data))

                  except Exception as e:
                      print(f"Error processing record {record['recordId']}: {str(e)}")

                      # Return error record for Firehose error handling
                      output.append({
                          'recordId': record['recordId'],
                          'result': 'ProcessingFailed'
                      })

              return {'records': output}


          def mask_sensitive_data(data):
              """
              Enhanced data masking for personal information protection
              """
              if isinstance(data, dict):
                  masked_data = {}
                  for key, value in data.items():
                      if isinstance(value, str):
                          masked_data[key] = mask_string_data(value)
                      elif isinstance(value, (dict, list)):
                          masked_data[key] = mask_sensitive_data(value)
                      else:
                          masked_data[key] = value
                  return masked_data
              elif isinstance(data, list):
                  return [mask_sensitive_data(item) for item in data]
              elif isinstance(data, str):
                  return mask_string_data(data)
              else:
                  return data

          def mask_string_data(data):
              """
              Apply string-level masking for email addresses and IP addresses
              """
              # Email address masking - complete domain masking
              email_pattern = r'\b([a-zA-Z0-9._%+-]+)@([a-zA-Z0-9.-]+\.[a-zA-Z]{2,})\b'

              def mask_email(match):
                  local = match.group(1)
                  domain = match.group(2)

                  # Local part masking (first character + asterisks)
                  if len(local) <= 1:
                      masked_local = local
                  else:
                      masked_local = local[0] + '*' * (len(local) - 1)

                  # Complete domain masking for AUTO version
                  masked_domain = '***.***'

                  return f"{masked_local}@{masked_domain}"

              masked_data = re.sub(email_pattern, mask_email, data)

              # IP address masking - show only first octet
              ip_pattern = r'\b(\d{1,3})\.(\d{1,3})\.(\d{1,3})\.(\d{1,3})\b'
              masked_data = re.sub(ip_pattern, r'\1.*.*.*', masked_data)

              return masked_data

          def send_metrics(access_level, data_size):
              """
              Send processing metrics to CloudWatch
              """
              try:
                  cloudwatch = boto3.client('cloudwatch')
                  cloudwatch.put_metric_data(
                      Namespace='SES/DataTransform',
                      MetricData=[
                          {
                              'MetricName': 'ProcessedDataSize',
                              'Value': data_size,
                              'Unit': 'Bytes',
                              'Dimensions': [
                                  {
                                      'Name': 'AccessLevel',
                                      'Value': access_level
                                  },
                                  {
                                      'Name': 'Environment',
                                      'Value': os.environ['ENVIRONMENT']
                                  }
                              ]
                          },
                          {
                              'MetricName': 'ProcessedRecords',
                              'Value': 1,
                              'Unit': 'Count',
                              'Dimensions': [
                                  {
                                      'Name': 'AccessLevel',
                                      'Value': access_level
                                  },
                                  {
                                      'Name': 'Environment',
                                      'Value': os.environ['ENVIRONMENT']
                                  }
                              ]
                          }
                      ]
                  )
              except Exception as e:
                  print(f"Failed to send metrics: {str(e)}")
      Tags:
        - Key: Name
          Value: !Sub '${ProjectCode}-${Environment}-data-transform'
        - Key: Environment
          Value: !Ref Environment
        - Key: Service
          Value: 'ses-migration'

  # Enhanced Kinesis Data Firehose Delivery Stream for Raw Logs
  RawLogsFirehoseStream:
    Type: AWS::KinesisFirehose::DeliveryStream
    DependsOn:
      - UnifiedDataStream
      - EnhancedDataFirehoseRole
    Properties:
      DeliveryStreamName: !Sub '${ProjectCode}-${Environment}-raw-logs-stream'
      DeliveryStreamType: DirectPut
      ExtendedS3DestinationConfiguration:
        BucketARN: !Sub 'arn:aws:s3:::${RawLogsBucketName}'
        BufferingHints:
          IntervalInSeconds: 60
          SizeInMBs: 1
        CompressionFormat: UNCOMPRESSED
        EncryptionConfiguration:
          KMSEncryptionConfig:
            AWSKMSKeyARN: !Ref KMSKeyArn
        Prefix: !Sub 'raw-logs/environment=${Environment}/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/'
        ErrorOutputPrefix: !Sub 'errors/raw-logs/environment=${Environment}/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/!{firehose:error-output-type}/'
        RoleARN: !GetAtt EnhancedDataFirehoseRole.Arn
        ProcessingConfiguration:
          Enabled: true
          Processors:
            - Type: Lambda
              Parameters:
                - ParameterName: LambdaArn
                  ParameterValue: !GetAtt DataTransformRawLambda.Arn
                - ParameterName: NumberOfRetries
                  ParameterValue: 3
                - ParameterName: RoleArn
                  ParameterValue: !GetAtt EnhancedDataFirehoseRole.Arn
                - ParameterName: BufferSizeInMBs
                  ParameterValue: 1
                - ParameterName: BufferIntervalInSeconds
                  ParameterValue: 60
        CloudWatchLoggingOptions:
          Enabled: true
          LogGroupName: !Ref RawLogsFirehoseLogGroup
          LogStreamName: !Ref RawLogsFirehoseLogStream
      Tags:
        - Key: Name
          Value: !Sub '${ProjectCode}-${Environment}-raw-logs-stream'
        - Key: Environment
          Value: !Ref Environment
        - Key: Service
          Value: 'ses-migration'
        - Key: DataType
          Value: 'raw'

  # Enhanced Kinesis Data Firehose Delivery Stream for Masked Logs
  MaskedLogsFirehoseStream:
    Type: AWS::KinesisFirehose::DeliveryStream
    DependsOn:
      - UnifiedDataStream
      - EnhancedDataFirehoseRole
    Properties:
      DeliveryStreamName: !Sub '${ProjectCode}-${Environment}-masked-logs-stream'
      DeliveryStreamType: DirectPut
      ExtendedS3DestinationConfiguration:
        BucketARN: !Sub 'arn:aws:s3:::${MaskedLogsBucketName}'
        BufferingHints:
          IntervalInSeconds: 60
          SizeInMBs: 1
        CompressionFormat: UNCOMPRESSED
        EncryptionConfiguration:
          KMSEncryptionConfig:
            AWSKMSKeyARN: !Ref KMSKeyArn
        Prefix: !Sub 'masked-logs/environment=${Environment}/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/'
        ErrorOutputPrefix: !Sub 'errors/masked-logs/environment=${Environment}/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/!{firehose:error-output-type}/'
        RoleARN: !GetAtt EnhancedDataFirehoseRole.Arn
        ProcessingConfiguration:
          Enabled: true
          Processors:
            - Type: Lambda
              Parameters:
                - ParameterName: LambdaArn
                  ParameterValue: !GetAtt DataTransformMaskedLambda.Arn
                - ParameterName: NumberOfRetries
                  ParameterValue: 3
                - ParameterName: RoleArn
                  ParameterValue: !GetAtt EnhancedDataFirehoseRole.Arn
                - ParameterName: BufferSizeInMBs
                  ParameterValue: 1
                - ParameterName: BufferIntervalInSeconds
                  ParameterValue: 60
        CloudWatchLoggingOptions:
          Enabled: true
          LogGroupName: !Ref MaskedLogsFirehoseLogGroup
          LogStreamName: !Ref MaskedLogsFirehoseLogStream
      Tags:
        - Key: Name
          Value: !Sub '${ProjectCode}-${Environment}-masked-logs-stream'
        - Key: Environment
          Value: !Ref Environment
        - Key: Service
          Value: 'ses-migration'
        - Key: DataType
          Value: 'masked'

Outputs:
  UnifiedDataStreamName:
    Description: Unified Kinesis Data Stream name for data source
    Value: !Ref UnifiedDataStream
    Export:
      Name: !Sub '${ProjectCode}-${Environment}-UnifiedDataStream'

  UnifiedDataStreamArn:
    Description: Unified Kinesis Data Stream ARN for data source
    Value: !GetAtt UnifiedDataStream.Arn
    Export:
      Name: !Sub '${ProjectCode}-${Environment}-UnifiedDataStreamArn'

  EnhancedDataTransformLambdaArn:
    Description: Enhanced data transformation Lambda function ARN
    Value: !GetAtt DataTransformRawLambda.Arn
    Export:
      Name: !Sub '${ProjectCode}-${Environment}-EnhancedDataTransformLambda'

  EnhancedRawLogsFirehoseStreamName:
    Description: Enhanced raw logs Firehose stream name
    Value: !Ref RawLogsFirehoseStream
    Export:
      Name: !Sub '${ProjectCode}-${Environment}-EnhancedRawLogsFirehoseStream'

  EnhancedRawLogsFirehoseStreamArn:
    Description: Enhanced raw logs Firehose stream ARN
    Value: !GetAtt RawLogsFirehoseStream.Arn
    Export:
      Name: !Sub '${ProjectCode}-${Environment}-EnhancedRawLogsFirehoseStreamArn'

  EnhancedMaskedLogsFirehoseStreamName:
    Description: Enhanced masked logs Firehose stream name
    Value: !Ref MaskedLogsFirehoseStream
    Export:
      Name: !Sub '${ProjectCode}-${Environment}-EnhancedMaskedLogsFirehoseStream'

  EnhancedMaskedLogsFirehoseStreamArn:
    Description: Enhanced masked logs Firehose stream ARN
    Value: !GetAtt MaskedLogsFirehoseStream.Arn
    Export:
      Name: !Sub '${ProjectCode}-${Environment}-EnhancedMaskedLogsFirehoseStreamArn'

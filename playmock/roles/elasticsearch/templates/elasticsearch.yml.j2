# ======================== Elasticsearch Configuration =========================
#
# NOTE: Elasticsearch comes with reasonable defaults for most settings.
#       Before you set out to tweak and tune the configuration, make sure you
#       understand what are you trying to accomplish and the consequences.
#
# The primary way of configuring a node is via this file. This template lists
# the most important settings you may want to configure for a production cluster.
#
# Please consult the documentation for further information on configuration options:
# https://www.elastic.co/guide/en/elasticsearch/reference/index.html
#
# ---------------------------------- Cluster -----------------------------------
#
# Use a descriptive name for your cluster:
#
cluster.name: {{ es_cluster_name }}
ingest.geoip.downloader.enabled: true
ingest.geoip.downloader.eager.download: true
#
# ------------------------------------ Node ------------------------------------
#
# Use a descriptive name for the node:
#
node.name: {{ ansible_hostname|lower }}
#
# Add custom attributes to the node:
#
#node.attr.rack: r1
#
node.roles:
{% for role in es_default_node_roles %}
  - {{ role }}
{% endfor %}
{% if inventory_hostname in groups['elasticsearch_master'] %}
{% if es_master_is_also_data_node %}
  - data
{% endif %}
{% endif %}
# ----------------------------------- Paths ------------------------------------
#
# Path to directory where to store the data (separate multiple locations by comma):
#
path.data: {{ es_data_dir }}
#
# Path to log files:
#
path.logs: {{ es_log_dir }}
#
# ----------------------------------- Memory -----------------------------------
#
# Lock the memory on startup:
#
#bootstrap.memory_lock: true
#
# Make sure that the heap size is set to about half the memory available
# on the system and that the owner of the process is allowed to use this
# limit.
#
# Elasticsearch performs poorly when the system is swapping the memory.
#
# ---------------------------------- Network -----------------------------------
#
# By default Elasticsearch is only accessible on localhost. Set a different
# address here to expose this node on the network:
#
network.host: 0.0.0.0
network.publish_host: {{ elastic_ip_address }}
#
# By default Elasticsearch listens for HTTP traffic on the first free port it
# finds starting at 9200. Set a specific HTTP port here:
#
http.port: 9200
#
# For more information, consult the network module documentation.
#
# --------------------------------- Discovery ----------------------------------
#
# Pass an initial list of hosts to perform discovery when this node is started:
# The default list of hosts is ["127.0.0.1", "[::1]"]
#
{% if groups['elasticsearch'] | length > 1  %}
discovery.type: "multi-node"
discovery.seed_hosts:
{% for host in groups['elasticsearch'] %}
  - {{ host }}
{% endfor %}
cluster.initial_master_nodes:
{% for host in groups['elasticsearch_master'] %}
  - {{ host }}
{% endfor %}
{% else %}
discovery.type: "single-node"
{% endif %}
#
# ---------------------------------- Various -----------------------------------
#
# Allow wildcard deletion of indices:
#
#action.destructive_requires_name: false

# ---------------------------------- Security -----------------------------------

xpack.security.enrollment.enabled: true

xpack.security.http.ssl.enabled: true
xpack.security.http.ssl.verification_mode: certificate
xpack.security.http.ssl.key: {{ es_certs_dir }}/{{ es_ca_filename }}
xpack.security.http.ssl.certificate: {{ es_certs_dir }}/{{ es_cert_filename }}
xpack.security.http.ssl.certificate_authorities: {{ es_certs_dir }}/{{ es_cert_filename }}

xpack.security.transport.ssl.enabled: true
xpack.security.transport.ssl.key: {{ es_certs_dir }}/{{ es_ca_filename }}
xpack.security.transport.ssl.certificate: {{ es_certs_dir }}/{{ es_cert_filename }}
xpack.security.transport.ssl.certificate_authorities: {{ es_certs_dir }}/{{ es_cert_filename }}


# ---------------------------------- Others --------------------------------------

action.auto_create_index: .monitoring*,.watches,.triggered_watches,.watcher-history*,.ml*,fluentd*

cluster.max_shards_per_node: 32000
